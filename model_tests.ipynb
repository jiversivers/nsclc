{
 "cells": [
  {
   "cell_type": "code",
   "id": "5e7fd5e92d90f688",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-23T20:45:13.076757Z",
     "start_time": "2024-05-23T20:45:09.411191Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "import importlib\n",
    "\n",
    "import my_modules.nsclc\n",
    "import my_modules.nsclc.nsclc_dataset\n",
    "import my_modules.custom_models.classifier_models\n",
    "importlib.reload(my_modules.nsclc.nsclc_dataset)\n",
    "from my_modules.nsclc.nsclc_dataset import NSCLCDataset\n",
    "importlib.reload(my_modules.custom_models.classifier_models)\n",
    "from my_modules.custom_models.classifier_models import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-23T20:45:13.076757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "data = NSCLCDataset('E:\\\\NSCLC Data - PMD', ['orr', 'taumean', 'boundfraction'], label='M', mask_on=False)\n",
    "data.normalize_channels_to_max()\n",
    "data.augment()\n",
    "data.show_random()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache reset\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Metastatic Potential Classification\n",
    "## Image-based, custom classifier models\n",
    "### Pre-trained performance evaluation\n",
    "The following tests are performed on models trained over 125 epochs with a learning rate of 0.01 on augmented, normalized, and unmasked data composed of orr, mean lifetime, and bound fraction images."
   ],
   "id": "bcf6e4b60e9ebbb3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "models = {'MLP': \n",
    "              [MLPNet, 'MLP Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'Parallel MLP': \n",
    "              [ParallelMLPNet, 'Parallel MLP_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'Regularized MLP': \n",
    "              [RegularizedMLPNet, 'Regularized MLP Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'Regularized Parallel MLP': \n",
    "              [RegularizedParallelMLPNet, 'Regularized Parallel MLP Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'CNN':\n",
    "              [CNNet, 'CN Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'Parallel CNN': \n",
    "              [ParallelCNNet, 'Parallel CN Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth'],\n",
    "          'Regularized Parallel CNN': \n",
    "              [RegularizedParallelCNNet, 'Regularized Parallel CN Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Augmented_Normalized.pth']\n",
    "          }"
   ],
   "id": "ab9786231a3ba4c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name, (model_fn, model_path) in models.items():\n",
    "    # Testing on dataset\n",
    "    model = model_fn(data.shape)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'models/{model_path}',  map_location=torch.device('cpu')))\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    data_out = []\n",
    "    data_pred = []\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            out = model(sample[0].unsqueeze(0)).item()\n",
    "            data_out.append(out)\n",
    "            pred = np.round(out)\n",
    "            data_pred.append(pred)\n",
    "            correct += 1 if pred == sample[1].item() else 0\n",
    "    print(f'Accuracy of {model_name} over whole dataset: {100*correct / len(data):.2f}%')"
   ],
   "id": "4944806226622cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Trash in $\\rightarrow$ Trash out\n",
    "As a sanity check, we have pushed through a \"random\" trash dataset below. We expect that trash in should give trash out. If it does, that is not a strong indication of model efficacy -- that will be addressed next. If it doesn't however, that is a strong indication of model failure.\n",
    "\n",
    "Trash out we expect to look like a roughly random chance prediction, meaning about half of the predictions will be in either class. If this number starts to shift too high or low for the class, that means the model defaults to a certain class and may be only guessing once class most of the time. Fortunately, our data are failry well balanced (45/55% split), so if this happens on the actual dataset, the performance is limited to 55%. "
   ],
   "id": "31a5e81ee6db29d1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def trash_in(model, trash_heap_size=500):\n",
    "    _in = torch.rand(trash_heap_size, *model.input_size)\n",
    "    trash_out = []\n",
    "    model.eval()\n",
    "    for trash in _in:\n",
    "        _out = model(trash.unsqueeze(0)).item()\n",
    "        trash_out.append(_out)\n",
    "    return trash_out"
   ],
   "id": "ad8f3da0b323ae4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name, (model_fn, model_path) in models.items():\n",
    "    model = model_fn(data.shape)\n",
    "    model.load_state_dict(torch.load(f'models/{model_path}',  map_location=torch.device('cpu')))\n",
    "    trash_out = trash_in(model, 500)\n",
    "    print(f'Trash into {model_name} resulted in {100*sum(trash_out) / len(trash_out):.2f}% of outputs being positively classified')"
   ],
   "id": "176dbacf3dca99af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5-Fold Cross Validation\n",
    "To assess efficacy, We will take the best performing models from this group (parallel MLPs and parallel CNNs) and perform 5-Fold Cross Validation to obtain more robust measures of performance for these models. For each fold, we will score raw accuracy and ROC-AUC."
   ],
   "id": "adca0a972d64ee17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Functions\n",
    "Defining some useful functions so we can limit code copying"
   ],
   "id": "261270a268192dcb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from my_modules.model_learning import test_model\n",
    "from my_modules.model_learning import fold_cross_validate\n",
    "from my_modules.model_learning.model_phases import trash_in\n",
    "\n",
    "# Creates folds by random sampling then multiplying samples by dataset numbers to augment without repeating slides in any group\n",
    "from my_modules.model_learning.loader_maker import fold_augmented_data"
   ],
   "id": "86ee0778944556db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we can use those functions to easily test our best-performers.",
   "id": "def1f4c0ef02fb07"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_models = [RegularizedParallelMLPNet, RegularizedParallelCNNet]\n",
    "data_folds = fold_augmented_data(data)\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer_fn = torch.optim.SGD\n",
    "for model_fn in best_models:\n",
    "    accuracy, running_loss, _  = fold_cross_validate(model_fn, data_folds, epochs=125, learning_rate=0.01)"
   ],
   "id": "1e78cab1f7f69e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also go ahead and run this test again with the masks on. If any of these models perform notably well, we can select them out and run 5-Fold Cross Validation across multiple hyperparameter sets to really fine tune things.",
   "id": "1d30410e96efefc5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data.mask_on = True\n",
    "for model_fn in best_models:\n",
    "    accuracy, running_loss, _  = fold_cross_validate(model_fn, data_folds, epochs=125, learning_rate=0.01)"
   ],
   "id": "e7e403d8cb58b4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Histogram-based custom classifier models\n",
    "First, we need to transform the dataset to match how the upcoming modes were trained. We can also take a look at some random samples to get an idea of what the model sees."
   ],
   "id": "3acc52216a9e6690"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data.dist_transform(nbins=25)\n",
    "data.mask_on = False\n",
    "data.show_random()"
   ],
   "id": "30dd4b413d16344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing Pre-trained Models\n",
    "These models are from a training session on the HPC. They were trained over varius hyperparameters and on the GPU. Some models were clearly better than others; accordingly, we already selected out the best performing architectures and are now comparing those models over a the gamut of hyperparams. "
   ],
   "id": "135452a44d72a408"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "models = {'Fast MLP': \n",
    "              [MLPNet, 'MLP Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          'Medium MLP': \n",
    "              [MLPNet, 'MLP Net_Epochs-250_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          'Slow MLP': \n",
    "              [MLPNet, 'MLP Net_Epochs-500_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          'Fast RNN':\n",
    "              [RNNet, 'RN Net_Epochs-125_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          'Medium RNN': \n",
    "              [RNNet, 'RN Net_Epochs-250_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          'Slow RNN': \n",
    "              [RNNet, 'RN Net_Epochs-500_nsclc_Metastases_orr+taumean+boundfraction_Transformed_Augmented_Normalized.pth'],\n",
    "          }"
   ],
   "id": "6624f68094c5c3df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name, (model_fn, model_path) in models.items():\n",
    "    # Testing on dataset\n",
    "    model = model_fn(data.shape)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'models/{model_path}', map_location=torch.device('cpu')))\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    data_out = []\n",
    "    data_pred = []\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            out = model(sample[0].unsqueeze(0)).item()\n",
    "            data_out.append(out)\n",
    "            pred = np.round(out)\n",
    "            data_pred.append(pred)\n",
    "            correct += 1 if pred == sample[1].item() else 0\n",
    "    print(f'Accuracy of {model_name} over whole dataset: {100*correct / len(data):.2f}%')"
   ],
   "id": "701fe6e0f1b057dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Trash in $\\rightarrow$ Trash out Test",
   "id": "efebd74f552eae5c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name, (model_fn, model_path) in models.items():\n",
    "    model = model_fn(data.shape)\n",
    "    model.load_state_dict(torch.load(f'models/{model_path}',  map_location=torch.device('cpu')))\n",
    "    trash_out = trash_in(model, 500)\n",
    "    print(f'Trash into {model_name} resulted in {100*sum(trash_out) / len(trash_out):.2f}% of outputs being positivley classified')"
   ],
   "id": "4a7bfbe23d4ba603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5-Fold Cross Validation \n",
    "First, we will use the basic and fast hyperparameters of 125 epochs with a learning rate of 0.01. "
   ],
   "id": "b009e3ac511d394f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "best_models = [MLPNet, ParallelMLPNet, RNNet]\n",
    "lengths = [int(0.2 * len(data)) for _ in range(5)]\n",
    "data_folds = torch.utils.data.random_split(dataset=data, lengths=lengths)"
   ],
   "id": "a47e332622bf03c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_fn in best_models:\n",
    "    accuracy, running_loss, _ = fold_cross_validate(model_fn, data_folds, epochs=125, learning_rate=0.01)"
   ],
   "id": "177cb8aa661fe25d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we will iterate through a gamut of hyperparameters to try to find the best combination. Becasue the histogrma transformation saves so much model-complexity, we can afford to do this for all models out of the gate.",
   "id": "8851d12b9dcbff85"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Try different hyperparameters in 5-Fold\n",
    "epochs = [125, 250, 500, 1000, 2500]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for ep in epochs:\n",
    "    for lr in learning_rates:\n",
    "        print(f'\\nEpochs: {ep} -- Learning Rate: {lr}\\n______________________________________')\n",
    "        for model_fn in best_models:\n",
    "            accuracy, running_loss, _ = fold_cross_validate(model_fn, data_folds, epochs=ep, learning_rate=lr)\n",
    "            print('______________________________________')"
   ],
   "id": "348b92c38556ca4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we will repeat this hyperparemeter iteration for the dataset with masks back on.",
   "id": "dd47bb278a855bbe"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data.mask_on = True\n",
    "for ep in epochs:\n",
    "    for lr in learning_rates:\n",
    "        print(f'\\nEpochs: {ep} -- Learning Rate: {lr}\\n______________________________________')\n",
    "        for model_fn in best_models:\n",
    "            accuracy, running_loss, _ = fold_cross_validate(model_fn, data_folds, epochs=ep, learning_rate=lr)\n",
    "            print('______________________________________')"
   ],
   "id": "1f2c6fcca0fddf74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# InceptionResNev2 + regularized MLP Classifier\n",
    "This will use the exact specification fo the model architecture form the included source. In short, InceptionResNetV2 is used to extract feature maps which are then averaged to obtain asort of \"feature spectrum.\" This is a nice way to add some depth to our model without adding a necessity for more data. InceptionResNetV2 is very deep (164 layers and 15 million parameers, I believe), but we aren't training it at all, so we don't have to worry about overfitting. We will just use the feature maps to train an MLP with 2 hidden layers each with dropouts to classify the liklihood of metastasis. This MP works very simililary to my Regualrized MLP Nets, but instead of outputting a binary class () or 1) it outputs a two-class liklihood (y, 1-(y)). To train, we will have to expand the original data labels to match this output, but functionally, it shouldn't change the question the model is trying to answer,. In fact this gives us better granualirty, as we will see driectly how certaiin the model is about the class prediciton.\n",
    "\n",
    "> Schiele S, Arndt TT, Martin B, et al. Deep Learning Prediction of Metastasis in Locally Advanced Colon Cancer Using Binary Histologic Tumor Images. Cancers (Basel). 2021;13(9):2074. Published 2021 Apr 25. doi:10.3390/cancers13092074"
   ],
   "id": "9087551fccce3315"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pretrainedmodels import inceptionresnetv2\n",
    "# NOTE: The automatica weight download for InceptionResNetV2 is broken. Model must be manually downloaded from https://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth and placed in C:/Users/<username>/.cache/torch/hub/checkpoints/\n",
    "# Setup the dataset for this model\n",
    "# Images, no mask (feature extractor will hopefully handle this), normalized (already is), \n",
    "data.mask_on = False\n",
    "data.dist_transformed = False\n",
    "\n",
    "batch_size = 21\n",
    "learning_rate = 0.0005\n",
    "optim_fn = torch.optim.RMSprop\n",
    "epochs = 300\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our base feature extractor and turn the gradients off -- we won't train it, just use it to feed our MLP.\n",
    "feature_extractor = inceptionresnetv2(num_classes=1000, pretrained='imagenet') # outputs 1536 feature maps when called with .features(x)\n",
    "for params in feature_extractor.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "\n",
    "# Dry run feature extractor to get output dims for classifier creation\n",
    "feature_extractor.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.rand(batch_size, data.shape[0], data.shape[1], data.shape[2])\n",
    "    dry_run = feature_extractor.features(x)\n",
    "feature_map_dims = dry_run.shape"
   ],
   "id": "5ed792b02f64f481",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Create classifier\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, input_size, feature_extractor):\n",
    "        super(classifier, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.input_size = input_size\n",
    "        self.feature_map_dims = self.get_features(torch.rand(1, *input_size))\n",
    "        \n",
    "        self.GlobalAvgPool = nn.AvgPool2d(feature_map_dims[-2::], stride=2) # Get average value for each feature map (1536 feature values/image)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1536, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        x = self.get_features(x)\n",
    "        \n",
    "        # Classify\n",
    "        x = self.GlobalAvgPool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        return self.feature_extractor(x)"
   ],
   "id": "fa8cafaf2045f572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Check that shapes are as expected\n",
    "m = classifier(x.shape[1:], feature_extractor.features)\n",
    "out = m(x)\n",
    "print(f'Input shape {x.shape}')\n",
    "print(f'After feature extraction: {m.get_features(x).shape}')\n",
    "print(f'After classifier: {out.shape}')"
   ],
   "id": "b0cfb44a4e6f8ed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "models = []\n",
    "accuracy = []\n",
    "running_loss = []\n",
    "for fold, test_set in enumerate(data_folds):\n",
    "    # Make model, loaders, & optimizer for fold\n",
    "    model = classifier(data.shape, feature_extractor.features)\n",
    "    train_sets = [data_folds[index] for index in range(5) if index != fold]\n",
    "    train_set = torch.utils.data.ConcatDataset(train_sets)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    optimizer = optimizer_fn(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        print(f'Epoch: {ep}')\n",
    "        for x, target in train_loader:\n",
    "            # Extract features\n",
    "            out = model(x)\n",
    "            \n",
    "            # Array to reform target to look like out\n",
    "            y = torch.zeros_like(out) \n",
    "            \n",
    "            # Put 100% certainty on the class\n",
    "            for r, t in enumerate(target):\n",
    "                y[r, t] = 1\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "    # Test\n",
    "    correct = 0\n",
    "    for x, target in test_loader:\n",
    "        out = model(x)\n",
    "        pred = torch.argmax(out)\n",
    "        correct += 1 if pred == target.item() else 0\n",
    "        accuracy.append(100*correct/len(test_loader.sampler))\n",
    "        models.append(model) \n",
    "        print(f'Inception + MLP accuracy for fold {fold+1}: {accuracy[-1]:.2f}%')\n",
    "print(f'>>>Inception + MLP average accuracy: {sum(accuracy)/len(accuracy):.2f}%<<<')"
   ],
   "id": "afbbfa269c453ab0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Manual Feature Extraction and Revision\n",
    "Using the best performing models from 5-Fold Cross Validation, we will do some rudimentary feature extraction to refine the models. Training for 1000 epochs with a learning rate of 0.01 on histogram data proved to be the best overall method, yielding the highest overall accuracy (MLP Net with accuracy of 87.82%) and yielding relatively high accuracies for all networks (all three in >80%). We will train a fresh MLP Net with those settings and then average the 5 models to use for further analysis."
   ],
   "id": "7e6a71eabefcfb1f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data.dist_transform()\n",
    "data.mask_on = False\n",
    "accuracy, running_loss, models = fold_cross_validate(MLPNet, data_folds, epochs=10, learning_rate=0.01)"
   ],
   "id": "89fd0cd44401c6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "summary(models[0], input_size=(batch_size,)+data.shape)"
   ],
   "id": "f2c762d75b1e5c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Get all params for each of the 5 models\n",
    "sd = [m.state_dict() for m in models]\n",
    "final_sd = {}\n",
    "\n",
    "# Average all parameters into final state dict\n",
    "for key in sd[0]:\n",
    "    final_sd[key] = (sd[0][key] + sd[1][key] + sd[2][key] + sd[3][key] + sd[4][key]) / 5\n",
    "\n",
    "# Create final model and run one final test on entire dataset\n",
    "final_model = MLPNet(data.shape)\n",
    "final_model.load_state_dict(final_sd)\n",
    "all_data_loader = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True)\n",
    "correct = test_model(final_model, all_data_loader)\n",
    "print(f'Accuracy on whole dataset: {100* correct / len(all_data_loader.sampler):0.2f}%')"
   ],
   "id": "70dd17db5083af3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This accuracy is much worse than the accuracy of any of the individual models over the entire dataset. This is a good indication that that the models are not neccessarily reaching a global minimum in the loss function. That's not a terrible thing, but it does mean that the model needs to be generalized carefully. To this end, we are going to test an ensemble of models (since its so stinking light weight) to see if that is better. Then, we will proceed with feature extraction. But before any of that, look at the performance of the models on the entire dataset:",
   "id": "4984504be3259d49"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model in models:\n",
    "    correct = test_model(model, all_data_loader)\n",
    "    print(f'Accuracy on whole dataset: {100* correct / len(all_data_loader.sampler):0.2f}%')"
   ],
   "id": "745cdf504f0e0220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# For now, just take the best model of the 5\n",
    "import matplotlib.pyplot as plt\n",
    "model = models[-1]\n",
    "sd = model.state_dict()\n",
    "weight_mat = sd['fc1.weight'].T\n",
    "plt.matshow(weight_mat, cmap=plt.cm.Accent)\n",
    "plt.show()\n",
    "\n",
    "weight_var = torch.var(weight_mat, axis=1)\n",
    "fig2 = plt.figure(figsize=(17, 4.5))\n",
    "plt.plot(weight_var)\n",
    "plt.show()\n",
    "print('test')"
   ],
   "id": "8488bda1ad02708c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now use the variance of weights for any single input node to determine (roughly) which are actually doing something dynamic and which are basically just bias values. The one with more variance are actually affecting relationships. The ones that have a low variance are just \"blanket\" a near-single value into all the next nodes and may be effectively learned by the bias instead. We can find the input nodes that are just blanket connected (e.g. nodes 30-50) and remove those. We can then retrain and see how much of a difference it makes. Our accuracy may drop, since we will be passing less data, but the model can be made lighter. If the input nodes were really not meaningful, our accuracy won't change meaningfully. This will help us find what is more important to the model.",
   "id": "b5581a19686906ab"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# The fraction of total input nodes we will keep\n",
    "fraction_to_take = 0.75\n",
    "num_to_take = int(len(weight_var) * fraction_to_take)\n",
    "\n",
    "# Create a list that pairs each weight variance with its original index so we can trace back after ranking\n",
    "val_idx = list(enumerate(weight_var))\n",
    "\n",
    "# Sort the pairs based on the value in descending order \n",
    "sorted_pairs = sorted(val_idx, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a dict that pairs where the value came from (index) with where it went (rank)\n",
    "rank_dict = {index: rank for rank, (index, value) in enumerate(sorted_pairs)}\n",
    "\n",
    "# Resort the ranks back into the matched locations using the original index\n",
    "weight_rank = [rank_dict[i] for i in range(len(weight_var))]\n",
    "\n",
    "# Look at the plot of just weights that made the cut\n",
    "sel_weights = np.array([weight for (weight, rank) in zip(weight_rank, weight_rank) if rank < num_to_take])\n",
    "fig = plt.matshow(sel_weights.T, cmap=plt.cm.Accent)\n",
    "plt.show()\n",
    "\n",
    "sel_weight_var = np.var(sel_weights.T, axis=0)\n",
    "fig2 = plt.figure(figsize=(17, 4.5))\n",
    "plt.plot(sel_weight_var)\n",
    "plt.show()"
   ],
   "id": "f3c6ebb26f5d256d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's also list the weights by rank to make sense of what values are important\n",
    "# First, we need to create a list of bin bounds based on the mode and nbins\n",
    "bin_width = 1 / 25\n",
    "bins = [[edge, edge + bin_width] for edge in np.arange(0, 1, bin_width)]\n",
    "\n",
    "# Then we rescale each value back up so we have individual bin values for each mode\n",
    "bin_values = [[[bin_value[0] * scalar, bin_value[1]* scalar] for bin_value in bins] for scalar in data.scalar]\n",
    "\n",
    "# Finally, make a label for the actual bin range for each respective mode\n",
    "bin_labels = [f'{mode} Bin: {low:0.2f} - {high:0.2f}' for mode, edge in zip(data.mode, bin_values) for low, high in edge]\n",
    "\n",
    "# Then we need to sort them based on the sorting of the weight variances of their respective bins\n",
    "sorted_labels = [bin_labels[rank] for rank in weight_rank]\n",
    "\n",
    "# Now we can show this\n",
    "for rank, label in enumerate(sorted_labels):\n",
    "    print(f'#{rank}: {label}')"
   ],
   "id": "6a9256b5fa0511ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# Retrain on selected features and test\n",
   "id": "f4e950f057e29584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Create ensemble of models\n",
    "correct = 0\n",
    "for x, target in all_data_loader:\n",
    "    out = torch.mean(torch.stack([m(x) for m in models]), dim=0) # Get individual model outputs and then average them \n",
    "    pred = torch.round(out) # Get final prediction from rounding\n",
    "    correct += torch.sum(pred.squeeze() == target).item()\n",
    "print(f'Ensemble accuracy over all data set: {100 * correct / len(all_data_loader.sampler):0.2f}%')"
   ],
   "id": "4c763f04e855dd52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is super good, so we need to do something to check it. Obviously this isn't very robust becasue we've seen all this data before, even though no one model has seen it all, any piece of data 4/5 models ahve trained on. So what we can do is hold out just a few samples from all training, then use those as a final test set for the ensemble.",
   "id": "dd6fecd7cbffb772"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "##### TODO: Handle the above #####",
   "id": "8383a65c27e21390",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model with all modes",
   "id": "f0616b77f8d42f18"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Prep hyperparams\n",
    "epochs = 125\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "optimizer_fn = torch.optim.SGD\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Prep data\n",
    "data.mode = ['all']\n",
    "data.mask_on = False\n",
    "data_folds = fold_augmented_data(data)\n",
    "model_fn = MLPNet\n",
    "accuracy, running_loss, models = fold_cross_validate(model_fn, data_folds, epochs=125, learning_rate=0.01)"
   ],
   "id": "f63ac43a27030230",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Off-the-shelf classifiers",
   "id": "93ee413016d3f24b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torchvision.models as tvm\n",
    "torchvision.models.list_models()"
   ],
   "id": "c6ac633285635918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "models = [tvm.mobilenet_v3_small, tvm.squeezenet1_1, tvm.alexnet, tvm.GoogLeNet]",
   "id": "dc89f1ec319b58e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_fn in models:\n",
    "    # Testing on dataset\n",
    "    model = model_fn()\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    data_out = []\n",
    "    data_pred = []\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            out = model(sample[0].unsqueeze(0))\n",
    "            pred = torch.argmax(out).item()\n",
    "            data_pred.append(pred)\n",
    "            correct += 1 if pred == sample[1].item() else 0\n",
    "    print(f'Accuracy of {type(model)} over whole dataset: {100*correct / len(data):.2f}%')"
   ],
   "id": "717f96f04da5b4b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_fn in models:\n",
    "    accuracy = []\n",
    "    running_loss = []\n",
    "    for fold, test_set in enumerate(data_folds):\n",
    "        model = model_fn()\n",
    "        train_sets = [data_folds[index] for index in range(5) if index != fold]\n",
    "        train_set = torch.utils.data.ConcatDataset(train_sets)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "        optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
    "        \n",
    "        model.train()\n",
    "        if torch.cuda.is_available() and not next(model.parameters()).is_cuda:\n",
    "            model.to(torch.device('cuda'))\n",
    "        total_loss = []\n",
    "        for epoch in range(125):\n",
    "            model.train(True)\n",
    "            for x, target in train_loader:\n",
    "                out = model(x).squeeze()\n",
    "                # Convert out from one-hot encoding\n",
    "                y = out/torch.sum(out)\n",
    "                y = out[target] / (1 - out[target])\n",
    "                loss = loss_fn(y, target)\n",
    "                total_loss += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss.append(total_loss)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for x, target in test_loader:\n",
    "            out = model(x).squeeze()\n",
    "            # Convert out from one-hot encoding\n",
    "            pred = torch.argmax(out).item()\n",
    "            correct += (pred == target).sum().item()\n",
    "        accuracy.append(100*correct/len(test_loader.sampler))\n",
    "        print(f'{model.name} accuracy for fold {fold+1}: {accuracy[-1]:.2f}%')\n",
    "    print(f'{model.name} average accuracy: {sum(accuracy)/len(accuracy):.2f}%')\n",
    "    plt.plot(running_loss)\n",
    "    plt.show()"
   ],
   "id": "98f982e332ac8ef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Results Decision\n",
    "While these results are poor, the training parameters for these models was not expected to be ideal at only 125 epochs and a high learning rate. From these models, we will choose the best performers of both MLPs (for both image and histograms), CNNs, and RNNs and retrain on an array of hyperparameters to find the optimized model training. From those, the best will be characterized for final results."
   ],
   "id": "4a8961a882f44b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results Summary for Image-based Classifiers\n",
    "While the MLP-based networks showed some promising results (with the parallel MLPs reaching accuracies >80%), the parallel CNNs outperform all MLP-based networks and are of comparable weight to a fully-connected MLP through the use of kernels rather than dense layers. Interestingly, a basic CNN was the worst performer by a notable margin. This may be due to the increased level of complexity that the model must learn to detect relationships across dimensions, while not adding appropriate numbers of trainable parameters or increased data-size. Both parallel models performed excellently with accuracies >90%. The regularized parallel network only slightly outperforms the parallel network, but adds no new trainable parameters (and only 9KB to the model size) and should make performance generally more stable.\n",
    "\n",
    "These models were all trained, evaluated, and tested originally on subsets of the entire dataset, but the tests shown here are across the entire dataset, meaning overfitting would be strongly reflected in the results. More robust checks for the best performers are below."
   ],
   "id": "f7e95d64c6302e09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

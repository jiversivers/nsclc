{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:06:56.077550Z",
     "start_time": "2024-07-01T22:06:56.071630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn.feature_selection\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import os\n",
    "import sys"
   ],
   "id": "a56fede8e9ec87af",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:06:57.462862Z",
     "start_time": "2024-07-01T22:06:56.711854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from my_modules.nsclc import NSCLCDataset\n",
    "import torchvision.transforms.v2 as tvt\n",
    "# Prepare data\n",
    "data = NSCLCDataset('E:/NSCLC_Data_for_ML',\n",
    "                    ['orr', 'g', 's', 'photons', 'tau1', 'tau2', 'alpha1', 'alpha2', 'taumean', 'boundfraction'],\n",
    "                    device='cpu', label='Metastases', mask_on=False)\n",
    "data.normalize_channels('preset')\n",
    "data.transforms = tvt.Compose([tvt.RandomVerticalFlip(p=0.25),\n",
    "                               tvt.RandomHorizontalFlip(p=0.25),\n",
    "                               tvt.RandomRotation(degrees=(-180, 180))])\n",
    "data.augment()"
   ],
   "id": "9b9e4b1971da04e7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-01T22:06:58.434530Z",
     "start_time": "2024-07-01T22:06:58.431453Z"
    }
   },
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:21:11.843061Z",
     "start_time": "2024-07-01T22:21:03.989237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Prepare folded data samplers\n",
    "# Get random indices of patients\n",
    "subsampler = torch.utils.data.sampler.SubsetRandomSampler(range(data.patient_count))\n",
    "idx = [i for i in subsampler]\n",
    "\n",
    "# Get the image indices for all patients as nested lists\n",
    "patient_subsets = [data.get_patient_subset(i) for i in idx]\n",
    "\n",
    "# Find and remove any patients with no image indices\n",
    "idx_for_removal = []\n",
    "for i, subset in enumerate(patient_subsets):\n",
    "    if len(subset) == 0:\n",
    "        idx_for_removal.append(idx[i])\n",
    "for ix in idx_for_removal:\n",
    "    idx.remove(ix)\n",
    "\n",
    "# Get labels for all remaining patients\n",
    "labels = [data.get_patient_label(i).item() for i in idx]\n",
    "\n",
    "# Separate 0 and 1 labels (still shuffled)\n",
    "shuffled_zeros = [i for i, l in zip(idx, labels) if l == 0]\n",
    "shuffled_ones = [i for i, l in zip(idx, labels) if l == 1]\n",
    "print('Number of non-metastatic patients: {}'.format(len(shuffled_ones)))\n",
    "print('Number of metastatic patients: {}'.format(len(shuffled_zeros)))\n",
    "\n",
    "train_subjects = shuffled_ones[3:] + shuffled_zeros[3:]\n",
    "train_subsets = [data.get_patient_subset(i) for i in train_subjects]  # Get all patient indices\n",
    "train_indices = [i for sub in train_subsets for i in sub]  # Un-nest\n",
    "random.shuffle(train_indices)\n",
    "\n",
    "test_subjects = shuffled_zeros[:3] + shuffled_ones[:3]  # Get a set of patients from both classes\n",
    "test_subsets = [data.get_patient_subset(i) for i in test_subjects]  # Get all patient indices\n",
    "test_indices = [i for sub in test_subsets for i in sub]  # Un-nest\n",
    "random.shuffle(test_indices)\n",
    "\n",
    "x_train = np.empty(((len(train_indices),) + data.shape))\n",
    "y_train = np.empty(len(train_indices))\n",
    "for i, idx in enumerate(train_indices):\n",
    "    x_train[i] = data[idx][0].numpy()\n",
    "    y_train[i] = data[idx][1].item()\n",
    "\n",
    "x_test = np.empty(((len(test_indices),) + data.shape))\n",
    "y_test = np.empty(len(test_indices))\n",
    "for i, idx in enumerate(test_indices):\n",
    "    x_test[i] = data[idx][0].numpy()\n",
    "    y_test[i] = data[idx][1].item()"
   ],
   "id": "851b67df1434f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-metastatic patients: 12\n",
      "Number of metastatic patients: 11\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:38:51.005486Z",
     "start_time": "2024-07-01T22:38:50.584548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_train[np.isnan(x_train)] = 0\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "x_test[np.isnan(x_test)] = 0\n",
    "print(x_train.shape)"
   ],
   "id": "9a3a2cfd4e887a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(485, 655360) (485,)\n",
      "(485, 655360)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T00:00:24.387123Z",
     "start_time": "2024-07-01T22:45:16.532551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit each model and test\n",
    "score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    pipe = make_pipeline(StandardScaler(), clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "    score[name] = pipe.score(x_test, y_test)\n",
    "    print(score[name])"
   ],
   "id": "bfdc83db5c65753d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4222222222222222\n",
      "0.4666666666666667\n",
      "0.5\n",
      "0.5\n",
      "0.5166666666666667\n",
      "0.45\n",
      "0.6111111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdivers\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45555555555555555\n",
      "0.3611111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdivers\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48333333333333334\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T14:31:26.987900Z",
     "start_time": "2024-07-02T14:31:26.975068Z"
    }
   },
   "cell_type": "code",
   "source": "print(score)",
   "id": "d894cd54f87bcb0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nearest Neighbors': 0.4222222222222222, 'Linear SVM': 0.4666666666666667, 'RBF SVM': 0.5, 'Gaussian Process': 0.5, 'Decision Tree': 0.5166666666666667, 'Random Forest': 0.45, 'Neural Net': 0.6111111111111112, 'AdaBoost': 0.45555555555555555, 'Naive Bayes': 0.3611111111111111, 'QDA': 0.48333333333333334}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:06:36.866712Z",
     "start_time": "2024-07-02T15:06:29.739265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=data.stack_height)\n",
    "pca.fit(x_train)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ],
   "id": "987b160e9a7afbcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30460095 0.10698153 0.07496978 0.02175497 0.02089734 0.01909765\n",
      " 0.0166035  0.01534505 0.01344136 0.01224451]\n",
      "[5300256.38409434 3141128.08711531 2629507.22853761 1416480.27078047\n",
      " 1388279.12398873 1327153.76970146 1237459.83259843 1189639.50428946\n",
      " 1113404.08142886 1062678.81823199]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.12 TiB for an array with shape (655360, 655360) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(pca\u001B[38;5;241m.\u001B[39mexplained_variance_ratio_)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(pca\u001B[38;5;241m.\u001B[39msingular_values_)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mpca\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:744\u001B[0m, in \u001B[0;36mPCA.score\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    724\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the average log-likelihood of all samples.\u001B[39;00m\n\u001B[0;32m    725\u001B[0m \n\u001B[0;32m    726\u001B[0m \u001B[38;5;124;03mSee. \"Pattern Recognition and Machine Learning\"\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    741\u001B[0m \u001B[38;5;124;03m    Average log-likelihood of the samples under the current model.\u001B[39;00m\n\u001B[0;32m    742\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    743\u001B[0m xp, _ \u001B[38;5;241m=\u001B[39m get_namespace(X)\n\u001B[1;32m--> 744\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(xp\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[1;32m~\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:718\u001B[0m, in \u001B[0;36mPCA.score_samples\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    716\u001B[0m Xr \u001B[38;5;241m=\u001B[39m X \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_\n\u001B[0;32m    717\u001B[0m n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m--> 718\u001B[0m precision \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_precision\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    719\u001B[0m log_like \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m xp\u001B[38;5;241m.\u001B[39msum(Xr \u001B[38;5;241m*\u001B[39m (Xr \u001B[38;5;241m@\u001B[39m precision), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    720\u001B[0m log_like \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m (n_features \u001B[38;5;241m*\u001B[39m log(\u001B[38;5;241m2.0\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mpi) \u001B[38;5;241m-\u001B[39m fast_logdet(precision))\n",
      "File \u001B[1;32m~\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_base.py:100\u001B[0m, in \u001B[0;36m_BasePCA.get_precision\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     98\u001B[0m precision \u001B[38;5;241m=\u001B[39m components_ \u001B[38;5;241m@\u001B[39m components_\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoise_variance_\n\u001B[0;32m     99\u001B[0m _add_to_diagonal(precision, \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m exp_var_diff, xp)\n\u001B[1;32m--> 100\u001B[0m precision \u001B[38;5;241m=\u001B[39m \u001B[43mcomponents_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlinalg_inv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprecision\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcomponents_\u001B[49m\n\u001B[0;32m    101\u001B[0m precision \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoise_variance_\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    102\u001B[0m _add_to_diagonal(precision, \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoise_variance_, xp)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.12 TiB for an array with shape (655360, 655360) and data type float64"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:22:42.934844Z",
     "start_time": "2024-07-02T15:21:04.038863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit each model and test\n",
    "score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    pipe = make_pipeline(StandardScaler(), PCA(n_components=data.stack_height), clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "    score[name] = pipe.score(x_test, y_test)\n",
    "    print(name, score[name])"
   ],
   "id": "8b5358c9018f5179",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.4777777777777778\n",
      "Linear SVM 0.46111111111111114\n",
      "RBF SVM 0.5\n",
      "Gaussian Process 0.5\n",
      "Decision Tree 0.40555555555555556\n",
      "Random Forest 0.37222222222222223\n",
      "Neural Net 0.5333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdivers\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 0.48333333333333334\n",
      "Naive Bayes 0.46111111111111114\n",
      "QDA 0.45555555555555555\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:21:04.037870Z",
     "start_time": "2024-07-02T15:19:35.065025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit each model and test\n",
    "score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    pipe = make_pipeline(StandardScaler(), PCA(n_components=3), clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "    score[name] = pipe.score(x_test, y_test)\n",
    "    print(name, score[name])"
   ],
   "id": "dca85fc9b43688ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.4222222222222222\n",
      "Linear SVM 0.5\n",
      "RBF SVM 0.5\n",
      "Gaussian Process 0.42777777777777776\n",
      "Decision Tree 0.3888888888888889\n",
      "Random Forest 0.45555555555555555\n",
      "Neural Net 0.42777777777777776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdivers\\PycharmProjects\\NSCLC_Classification\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 0.46111111111111114\n",
      "Naive Bayes 0.4722222222222222\n",
      "QDA 0.5777777777777777\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit each model and test\n",
    "score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    pipe = make_pipeline(StandardScaler(), PCA(n_components=3), clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "    score[name] = pipe.score(x_test, y_test)\n",
    "    print(name, score[name])"
   ],
   "id": "18f39595163fd849"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
